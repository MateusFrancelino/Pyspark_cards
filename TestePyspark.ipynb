{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import Row\n",
    "from pyspark.sql.types import StructType, StructField, StringType, DateType,DecimalType\n",
    "from pyspark.sql.functions import udf, current_date,  to_date, date_format, lit, col\n",
    "from pyspark.sql.types import BinaryType\n",
    "import uuid\n",
    "import datetime\n",
    "\n",
    "import pandas as pd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "connection_url = None\n",
    "user = None\n",
    "password = None\n",
    "directory = None\n",
    "bronze_directory = None\n",
    "silver_directory = None\n",
    "start_pattern = None\n",
    "end_extension = None\n",
    "notebook_name = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['JAVA_HOME'] = r'C:\\Program Files\\Java\\jdk1.8.0_202'\n",
    "os.environ['PATH'] = os.environ['JAVA_HOME'] + r'\\bin;' + os.environ['PATH']\n",
    "driver_path = \"C:/ocDB/WINDOWS.X64_193000_db_home/jdbc/lib/ojdbc8.jar\"\n",
    "if connection_url is None:\n",
    "    connection_url = \"jdbc:oracle:thin:@//localhost:1521/orcl\"\n",
    "if user is None:\n",
    "    user = \"sys as SYSDBA\"\n",
    "if password is None:\n",
    "    password = \"root\"\n",
    "if directory is None:\n",
    "    directory = \"Volumes/dev/tiintegracao/team/cartoes/cext\"\n",
    "if bronze_directory is None:\n",
    "    bronze_directory = \"Volumes/bronze/\"\n",
    "if silver_directory is None:\n",
    "    silver_directory = \"Volumes/silver/\"\n",
    "if start_pattern is None:\n",
    "    start_pattern = \"CEXT_756\"\n",
    "if end_extension is None:\n",
    "    end_extension = \".CCB\"\n",
    "if notebook_name is None:\n",
    "    notebook_name = \"TestePyspark.ipynb\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Exemplo de Spark JDBC com Oracle\") \\\n",
    "    .config(\"spark.driver.extraClassPath\", driver_path) \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_uuid(df,col_name):\n",
    "    \n",
    "    def uuid_bytes():\n",
    "        return uuid.uuid4().bytes\n",
    "\n",
    "    \n",
    "    udf_uuid_bytes = udf(uuid_bytes, BinaryType())\n",
    "\n",
    "    \n",
    "    df_with_uuid = df.withColumn(col_name, udf_uuid_bytes())\n",
    "\n",
    "    return df_with_uuid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_log(status, message):\n",
    "    log_file= bronze_directory+\"log.csv\"\n",
    "    new_log = pd.DataFrame({\n",
    "        'Status': [status],\n",
    "        'Date': [datetime.datetime.now().strftime('%Y/%m/%d %H:%M:%S')],\n",
    "        'Notebook': [notebook_name],\n",
    "        'Message': [message]\n",
    "    })\n",
    "    \n",
    "    if os.path.exists(log_file):\n",
    "        log_df = pd.read_csv(log_file)\n",
    "        log_df = pd.concat([log_df, new_log], ignore_index=True)\n",
    "    else:\n",
    "        log_df = new_log\n",
    "    \n",
    "    log_df.to_csv(log_file, index=False)\n",
    "    schema = StructType([\n",
    "        StructField(\"status\", StringType(), True),\n",
    "        StructField(\"notebook\", StringType(), True),\n",
    "        StructField(\"message\", StringType(), True),\n",
    "        StructField(\"dhlog\", DateType(), True)\n",
    "    ])\n",
    "    df_log = spark.createDataFrame([(status, notebook_name, message, datetime.datetime.now().now().date())], schema)\n",
    "    df_log = add_uuid(df_log,'id')\n",
    "    df_log.show()\n",
    "    df_log.write.jdbc(\n",
    "                url=connection_url,\n",
    "                table=\"BRONZE.log_table\",\n",
    "                mode=\"append\",\n",
    "                properties={\"user\": user, \"password\": password, \"driver\": \"oracle.jdbc.driver.OracleDriver\"}\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def append_to_csv(df, csv_path):\n",
    "    csv_path =  silver_directory + csv_path\n",
    "    if os.path.exists(csv_path):\n",
    "        \n",
    "        existing_df = pd.read_csv(csv_path)\n",
    "        df = pd.concat([existing_df, df], ignore_index=True)\n",
    "    \n",
    "    df.to_csv(csv_path, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def queryData(query):\n",
    "    df = spark.read.format(\"jdbc\") \\\n",
    "        .option(\"url\", connection_url) \\\n",
    "        .option(\"driver\", \"oracle.jdbc.OracleDriver\") \\\n",
    "        .option(\"query\", query) \\\n",
    "        .option(\"user\", user) \\\n",
    "        .option(\"password\", password) \\\n",
    "        .load()\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_file(filepath,file_name):\n",
    "    rows = []\n",
    "    with open(filepath, 'r') as file:\n",
    "        lines = file.readlines()\n",
    "        line_count = 1\n",
    "        for line in lines[1:-1]: \n",
    "            card_number = line[6:25].strip() \n",
    "            date = line[34:42].strip()\n",
    "            rows.append({ \n",
    "                          \"card_number\": card_number,\n",
    "                          \"date\": date,\n",
    "                          \"line_content\":line,\n",
    "                          \"file_name\":file_name,\n",
    "                          \"line_number\":line_count})\n",
    "            line_count+=1\n",
    "    return rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def filter_files(directory, start_pattern, end_extension):\n",
    "    filtered_files = []\n",
    "    already_read_files = set()\n",
    "    readed_files_directory = bronze_directory+\"readed_cxt\"\n",
    "    if os.path.exists(readed_files_directory):\n",
    "        with open(readed_files_directory, 'r') as file:\n",
    "            already_read_files = set(line.strip() for line in file.readlines())\n",
    "\n",
    "    new_files = []\n",
    "\n",
    "    for filename in os.listdir(directory):\n",
    "        if filename.startswith(start_pattern) and filename.endswith(end_extension):\n",
    "            filepath = os.path.join(directory, filename)\n",
    "            if filename in already_read_files:\n",
    "                continue\n",
    "            \n",
    "            with open(filepath, 'r') as file:\n",
    "                lines = file.readlines()\n",
    "                if lines[0].startswith(\"CEXT0\") and lines[-1].startswith(\"CEXT9\"):\n",
    "                    filtered_files.append(filename)\n",
    "                    new_files.append(filename)\n",
    "\n",
    "    if new_files:\n",
    "        with open(readed_files_directory, 'a') as file:\n",
    "            for filename in new_files:\n",
    "                file.write(filename + '\\n')\n",
    "    \n",
    "    return filtered_files\n",
    "    \n",
    "    return filtered_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    files_list = filter_files(directory, start_pattern, end_extension)\n",
    "except Exception as e:\n",
    "    make_log('error', str(e))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['CEXT_7562011_20240125_0002504.CCB']"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(files_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    all_rows = []\n",
    "    for file in files_list:\n",
    "        all_rows.extend(process_file(directory+'/'+file,file))\n",
    "\n",
    "    with open(\"data.json\", \"w\") as f:\n",
    "        json.dump(all_rows, f)\n",
    "except Exception as e:\n",
    "    make_log('error', str(e))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    df_transactions = None\n",
    "    if all_rows:\n",
    "        df_transactions = spark.createDataFrame(all_rows)\n",
    "        df_transactions = df_transactions.withColumn(\"card_number\", df_transactions[\"card_number\"].cast(DecimalType(16, 0)))\n",
    "        \n",
    "except Exception as e:\n",
    "    make_log('error', str(e))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+--------+--------------------+--------------------+-----------+\n",
      "|     card_number|    date|           file_name|        line_content|line_number|\n",
      "+----------------+--------+--------------------+--------------------+-----------+\n",
      "|5151070044381239|20230612|CEXT_7562011_2024...|75600051510700443...|          1|\n",
      "|5151070044381239|20230612|CEXT_7562011_2024...|75600051510700443...|          2|\n",
      "|5151070044381239|20230612|CEXT_7562011_2024...|75600051510700443...|          3|\n",
      "|5151070432093151|20230612|CEXT_7562011_2024...|75600051510704320...|          4|\n",
      "|5151070432093151|20230612|CEXT_7562011_2024...|75600051510704320...|          5|\n",
      "|5151070432093151|20230612|CEXT_7562011_2024...|75600051510704320...|          6|\n",
      "|5151070432093151|20230612|CEXT_7562011_2024...|75600051510704320...|          7|\n",
      "|5151940230409696|20230612|CEXT_7562011_2024...|75600051519402304...|          8|\n",
      "|5151940230409696|20230612|CEXT_7562011_2024...|75600051519402304...|          9|\n",
      "|5151070044387015|20230612|CEXT_7562011_2024...|75600051510700443...|         10|\n",
      "+----------------+--------+--------------------+--------------------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_transactions.show() if df_transactions is not None else print(\"df_transactions is None: \",df_transactions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:    \n",
    "        if df_transactions is not None:\n",
    "                cards_number = df_transactions.select(col(\"card_number\").cast(\"string\")).distinct().rdd.flatMap(lambda x: x).collect()\n",
    "                df_cards = queryData('SELECT * FROM CARTOES.TB_CARTAO WHERE NRCARTAO IN ({})'.format(', '.join(cards_number)))\n",
    "                df_control = queryData(\"\"\"\n",
    "                        SELECT IDARQUIVO_CONTROLE \n",
    "                        FROM CARTOES.TB_ARQUIVO_CONTROLE \n",
    "                        WHERE NRARQUIVO = 2\n",
    "                        \"\"\")\n",
    "except Exception as e:\n",
    "    print(e)\n",
    "    make_log('error', str(e))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    if df_transactions is not None:\n",
    "        df_join = df_transactions.join(df_cards, df_transactions[\"card_number\"] == df_cards[\"NRCARTAO\"], \"inner\")\n",
    "        df_transactions = df_join.select(\"date\", \"card_number\",\"file_name\",\"line_content\",\"line_number\")\n",
    "        del df_join\n",
    "except Exception as e:\n",
    "    make_log('error', str(e))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    if df_transactions is not None:\n",
    "        df_transactions\n",
    "        df_transactions = df_transactions.withColumn('date_now', current_date()) \\\n",
    "                                        .withColumn(\"date\", to_date(df_transactions[\"date\"], \"yyyyMMdd\"))\n",
    "        df_transactions = df_transactions.withColumn(\"date\", date_format(df_transactions[\"date\"], \"dd/MM/yyyy\")) \\\n",
    "                                        .withColumn(\"date_now\", date_format(df_transactions[\"date_now\"], \"dd/MM/yyyy\"))\n",
    "\n",
    "        df_transactions = df_transactions.withColumn('IDARQUIVO', lit(uuid.uuid4().bytes))\n",
    "        df_transactions = df_transactions.withColumn('CDSITUACAO', lit(1))\n",
    "\n",
    "        df_transactions = add_uuid(df_transactions,\"IDARQUIVO_LINHA\")\n",
    "except Exception as e:\n",
    "    make_log('error', str(e))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----------------+--------------------+--------------------+-----------+----------+--------------------+----------+--------------------+\n",
      "|      date|     card_number|           file_name|        line_content|line_number|  date_now|           IDARQUIVO|CDSITUACAO|     IDARQUIVO_LINHA|\n",
      "+----------+----------------+--------------------+--------------------+-----------+----------+--------------------+----------+--------------------+\n",
      "|12/06/2023|5151070044381239|CEXT_7562011_2024...|75600051510700443...|          1|10/06/2024|[86 9E 12 59 19 E...|         1|[CC 1C 76 80 38 C...|\n",
      "|12/06/2023|5151070044381239|CEXT_7562011_2024...|75600051510700443...|          2|10/06/2024|[86 9E 12 59 19 E...|         1|[C3 A8 20 16 1E D...|\n",
      "|12/06/2023|5151070044381239|CEXT_7562011_2024...|75600051510700443...|          3|10/06/2024|[86 9E 12 59 19 E...|         1|[BE 35 E2 14 AF A...|\n",
      "|12/06/2023|5151070044387015|CEXT_7562011_2024...|75600051510700443...|         10|10/06/2024|[86 9E 12 59 19 E...|         1|[D0 BB CD 82 57 4...|\n",
      "+----------+----------------+--------------------+--------------------+-----------+----------+--------------------+----------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_transactions.show() if df_transactions is not None else print(\"df_transactions is None: \",df_transactions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    df_archive = None\n",
    "    df_archive_line = None\n",
    "    if df_transactions is not None:\n",
    "        df_archive = df_transactions.withColumnRenamed('file_name', 'NMARQUIVO') \\\n",
    "                                    .withColumnRenamed('date', 'DTARQUIVO') \\\n",
    "                                    .withColumnRenamed('date_now', 'DHREGISTRO') \\\n",
    "                                    .drop(\"card_number\") \\\n",
    "                                    .drop(\"line_content\") \\\n",
    "                                    .drop(\"IDARQUIVO_LINHA\") \\\n",
    "                                    .drop(\"line_number\") \\\n",
    "                                    .drop(\"CDSITUACAO\")\n",
    "                                    \n",
    "        df_archive_line = df_transactions.drop(\"card_number\")\\\n",
    "                                        .drop(\"file_name\")\\\n",
    "                                        .withColumnRenamed('line_content', 'DSCONTEUDO') \\\n",
    "                                        .withColumnRenamed('date', 'DTPROCESSO') \\\n",
    "                                        .withColumnRenamed('line_number', 'NRLINHA') \\\n",
    "                                        .withColumnRenamed('date_now', 'DHREGISTRO')\n",
    "except Exception as e:\n",
    "    make_log('error', str(e))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------------------+-------+----------+--------------------+----------+--------------------+\n",
      "|DTPROCESSO|          DSCONTEUDO|NRLINHA|DHREGISTRO|           IDARQUIVO|CDSITUACAO|     IDARQUIVO_LINHA|\n",
      "+----------+--------------------+-------+----------+--------------------+----------+--------------------+\n",
      "|12/06/2023|75600051510700443...|      1|10/06/2024|[86 9E 12 59 19 E...|         1|[E5 D8 17 05 EE 6...|\n",
      "|12/06/2023|75600051510700443...|      2|10/06/2024|[86 9E 12 59 19 E...|         1|[B0 61 11 37 67 6...|\n",
      "|12/06/2023|75600051510700443...|      3|10/06/2024|[86 9E 12 59 19 E...|         1|[87 5F B0 78 91 0...|\n",
      "|12/06/2023|75600051510700443...|     10|10/06/2024|[86 9E 12 59 19 E...|         1|[72 48 EC A5 B8 B...|\n",
      "+----------+--------------------+-------+----------+--------------------+----------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_archive_line.show() if df_transactions is not None else print(\"df_archive_line is None: \",df_archive_line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    if df_archive is not None:\n",
    "        df_archive = df_archive.crossJoin(df_control)\n",
    "        df_archive = df_archive.distinct()\n",
    "except Exception as e:\n",
    "    make_log('error', str(e))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------------------+----------+--------------------+--------------------+\n",
      "| DTARQUIVO|           NMARQUIVO|DHREGISTRO|           IDARQUIVO|  IDARQUIVO_CONTROLE|\n",
      "+----------+--------------------+----------+--------------------+--------------------+\n",
      "|12/06/2023|CEXT_7562011_2024...|10/06/2024|[86 9E 12 59 19 E...|[FF 95 CE C4 CE 7...|\n",
      "+----------+--------------------+----------+--------------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "df_archive.show() if df_transactions is not None else print(\"df_archive is None: \",df_archive)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    if df_archive is not None:\n",
    "        df_archive.write.jdbc(\n",
    "                url=connection_url,\n",
    "                table=\"CARTOES.TB_ARQUIVO\",\n",
    "                mode=\"append\",\n",
    "                properties={\"user\": user, \"password\": password, \"driver\": \"oracle.jdbc.driver.OracleDriver\"}\n",
    "            )\n",
    "        df_pandas = df_archive.toPandas()\n",
    "        append_to_csv(df_pandas, \"CARTOES/TB_ARQUIVO.csv\")\n",
    "except Exception as e:\n",
    "    print(e)\n",
    "    make_log('error', str(e))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    if df_archive_line is not None:\n",
    "        df_archive_line.write.jdbc(\n",
    "                url=connection_url,\n",
    "                table=\"CARTOES.TB_ARQUIVO_LINHA\",\n",
    "                mode=\"append\",\n",
    "                properties={\"user\": user, \"password\": password, \"driver\": \"oracle.jdbc.driver.OracleDriver\"}\n",
    "            )\n",
    "        df_pandas = df_archive_line.toPandas()\n",
    "        append_to_csv(df_pandas, \"CARTOES/TB_ARQUIVO_LINHA.csv\")\n",
    "except Exception as e:\n",
    "    make_log('error', str(e))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------------+--------------------+----------+--------------------+\n",
      "| status|          notebook|             message|     dhlog|                  id|\n",
      "+-------+------------------+--------------------+----------+--------------------+\n",
      "|success|TestePyspark.ipynb|notebook ran succ...|2024-06-10|[0C D6 84 7E 28 8...|\n",
      "+-------+------------------+--------------------+----------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "make_log(\"success\", \"notebook ran successfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'dhlog' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[38], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mdhlog\u001b[49m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'dhlog' is not defined"
     ]
    }
   ],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
